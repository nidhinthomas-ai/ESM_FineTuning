{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a50bb64-df4f-49f3-86e4-af1c47f13639",
   "metadata": {},
   "source": [
    "# Finetuning an LoRA of ESM-2 for Protein Binding Site Prediction\n",
    "\n",
    "This tutorial is based on the Hugging Face Article by AmelieSchreiber (https://huggingface.co/blog/AmelieSchreiber/esmbind), where she discusses the low-rank adaptation (LoRA) of ESM-2 for predicting protein binding site. The curated dataset can either be downloaded from Hugging Face directly or generated using the `data_preprocessing_notebook.ipynb`. \n",
    "\n",
    "## ESM-2 pLM: \n",
    "\n",
    "The ESM-2 protein language model, along with ESMFold, offers a significant advancement in protein sequence analysis by eliminating the need for Multiple Sequence Alignment (MSA) in its predictions. This simplification leads to a more user-friendly experience, requiring less specialized knowledge and enabling quicker results. Remarkably, these models achieve performance on par with or superior to AlphaFold2, yet they operate up to 60 times faster. Additionally, ESM-2 does not depend on structural data, which is particularly advantageous given that many proteins lack characterized 3D structures. The rapid structural predictions from ESMFold are contributing to a growing repository of protein structures, as seen in the expansive Metagenomic Atlas. Despite their impressive speed and accuracy, ESM-2 and its counterparts have not yet reached the popularity of AlphaFold2, but their value is increasingly recognized. Users with varying levels of expertise in deep learning and protein science are encouraged to explore these models, potentially enhancing their performance by training their own LoRA models or refining data, especially those familiar with databases like UniProt.\n",
    "\n",
    "## Low-Rank Adaptation of ESM-2:\n",
    "\n",
    "Low-Rank Adaptation (LoRA) of ESM-2, which stands for Evolutionary Scale Modeling 2, represents a cutting-edge approach to fine-tuning large-scale language models specifically tailored for biological sequences. LoRA introduces a novel method to adapt pre-trained models by updating only a small subset of model parameters, thereby significantly reducing the computational overhead typically associated with training such expansive networks. By focusing on the low-rank decomposition of weight matrices within the transformer layers, LoRA strategically modifies the model to capture task-specific nuances without the need for extensive retraining. This technique is particularly beneficial for ESM-2, a model designed to understand and predict protein structure and function, as it allows researchers to efficiently adapt the model to new datasets or emerging biological problems. \n",
    "\n",
    "The application of LoRA to ESM-2 fine-tuning can also serve as a potent strategy to mitigate overfitting, a common challenge when adapting large models to specific tasks with limited data. By constraining updates to a low-rank subspace of the model's parameters, LoRA effectively regularizes the fine-tuning process, reducing the model's capacity to memorize training data and instead promoting the learning of generalizable patterns. This targeted update approach not only preserves the rich representations learned during pre-training but also ensures that the fine-tuning does not deviate excessively from the original parameter space, which is instrumental in maintaining the model's robustness to unseen data. Consequently, LoRA enables ESM-2 to maintain its predictive power across diverse biological sequences, making it a valuable tool for researchers aiming to leverage deep learning without succumbing to the pitfalls of overfitting in specialized domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18df4778-b0b9-49b0-b6ce-c60a3aa1132d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    roc_auc_score, \n",
    "    matthews_corrcoef\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from accelerate import Accelerator\n",
    "# Imports specific to the custom peft lora model\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31fa4c0-f483-47b0-8301-65bb8b5e8215",
   "metadata": {},
   "source": [
    "## Defining metrics and losses:\n",
    "\n",
    "The cell below outlines a series of steps for preparing and processing protein sequence data for predicting the binding site in a protein from sequence information alone, specifically token classification using a pre-trained model like ESM-2. Initially, it defines helper functions to truncate labels to a maximum length, compute various evaluation metrics (accuracy, precision, recall, F1 score, AUC, and MCC), and calculate custom loss during training. The code then proceeds to load protein sequence data and corresponding labels from pickle files, which are presumably chunked by protein family for a more organized dataset.\n",
    "\n",
    "Tokenization is performed on the sequences using a tokenizer pre-trained on the ESM-2 model, with padding and truncation applied to ensure a consistent sequence length. The labels are truncated to match the tokenized sequences' length. These processed sequences and labels are then converted into datasets suitable for training and evaluation.\n",
    "\n",
    "Furthermore, the code calculates class weights to address potential imbalances in the label distribution, which can be crucial for ensuring fair representation during model training. These weights are adjusted for the hardware accelerator in use, which could be a CPU or GPU, to optimize computational performance. The Accelerator class from the accelerate library is used to facilitate this hardware optimization, ensuring that the class weights are compatible with the device being used for training.\n",
    "\n",
    "Download the .pkl dataset files from the [here](https://huggingface.co/datasets/AmelieSchreiber/binding_sites_random_split_by_family/tree/main).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b222c741-d95d-4456-97c0-8cdeb391cc29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper Functions and Data Preparation\n",
    "def truncate_labels(labels, max_length):\n",
    "    \"\"\"Truncate labels to the specified max_length.\"\"\"\n",
    "    return [label[:max_length] for label in labels]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Compute metrics for evaluation.\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove padding (-100 labels)\n",
    "    predictions = predictions[labels != -100].flatten()\n",
    "    labels = labels[labels != -100].flatten()\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    # Compute precision, recall, F1 score, and AUC\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    auc = roc_auc_score(labels, predictions)\n",
    "    \n",
    "    # Compute MCC\n",
    "    mcc = matthews_corrcoef(labels, predictions) \n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'auc': auc, 'mcc': mcc} \n",
    "\n",
    "def compute_loss(model, inputs):\n",
    "    \"\"\"Custom compute_loss function.\"\"\"\n",
    "    logits = model(**inputs).logits\n",
    "    labels = inputs[\"labels\"]\n",
    "    loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    active_loss = inputs[\"attention_mask\"].view(-1) == 1\n",
    "    active_logits = logits.view(-1, model.config.num_labels)\n",
    "    active_labels = torch.where(\n",
    "        active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "    )\n",
    "    loss = loss_fct(active_logits, active_labels)\n",
    "    return loss\n",
    "\n",
    "# Load the data from pickle files (replace with your local paths)\n",
    "with open(\"train_sequences_chunked_by_family.pkl\", \"rb\") as f:\n",
    "    train_sequences = pickle.load(f)\n",
    "\n",
    "with open(\"test_sequences_chunked_by_family.pkl\", \"rb\") as f:\n",
    "    test_sequences = pickle.load(f)\n",
    "\n",
    "with open(\"train_labels_chunked_by_family.pkl\", \"rb\") as f:\n",
    "    train_labels = pickle.load(f)\n",
    "\n",
    "with open(\"test_labels_chunked_by_family.pkl\", \"rb\") as f:\n",
    "    test_labels = pickle.load(f)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "max_sequence_length = 1000\n",
    "\n",
    "train_tokenized = tokenizer(train_sequences, padding=True, truncation=True, max_length=max_sequence_length, return_tensors=\"pt\", is_split_into_words=False)\n",
    "test_tokenized = tokenizer(test_sequences, padding=True, truncation=True, max_length=max_sequence_length, return_tensors=\"pt\", is_split_into_words=False)\n",
    "\n",
    "# Directly truncate the entire list of labels\n",
    "train_labels = truncate_labels(train_labels, max_sequence_length)\n",
    "test_labels = truncate_labels(test_labels, max_sequence_length)\n",
    "\n",
    "train_dataset = Dataset.from_dict({k: v for k, v in train_tokenized.items()}).add_column(\"labels\", train_labels)\n",
    "test_dataset = Dataset.from_dict({k: v for k, v in test_tokenized.items()}).add_column(\"labels\", test_labels)\n",
    "\n",
    "# Compute Class Weights\n",
    "classes = [0, 1]  \n",
    "flat_train_labels = [label for sublist in train_labels for label in sublist]\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=flat_train_labels)\n",
    "accelerator = Accelerator()\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(accelerator.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc6630-c4ed-4aca-8112-fd56826aa9f0",
   "metadata": {},
   "source": [
    "## Define Custom Trainer Class:\n",
    "\n",
    "In the context of protein sequence analysis, particularly for predicting binding site residues, datasets often exhibit a significant class imbalance where the number of non-binding residues vastly outnumbers the binding residues. This imbalance can skew the model's learning process, leading to suboptimal performance, as the model might become biased towards predicting the majority class. To address this issue, a custom `WeightedTrainer` class is defined, inheriting from the `Trainer` class. This custom trainer overrides the `compute_loss` method to incorporate a tailored loss function that takes into account the class weights computed from the dataset. By doing so, the model is penalized more for misclassifying the minority class, thereby encouraging the model to learn a more balanced representation of both binding and non-binding sites. This approach helps to improve the model's predictive accuracy on the underrepresented class, which is critical for tasks where identifying the less frequent, yet functionally significant, binding sites is the primary objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9540be7c-fdfc-4cc7-98ee-ca3b02eefff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        loss = compute_loss(model, inputs)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa2df9-3077-4939-9357-af89e451401e",
   "metadata": {},
   "source": [
    "## Training the LoRA ESM-2:\n",
    "\n",
    "The training function is designed to fine-tune the ESM-2 protein language model for the task of token classification, specifically identifying binding sites within protein sequences. It leverages the Low-Rank Adaptation (LoRA) technique, which allows for the adjustment of key hyperparameters such as the rank (`r`) and scaling factor (`lora_alpha`). Users are encouraged to experiment with these settings, as well as the choice of weight matrices to apply LoRA to, in order to optimize the model's performance on the dataset. The function sets up a training environment with a pre-defined configuration, including learning rate, learning rate scheduler, and gradient clipping, among others. It uses a custom `WeightedTrainer` class that accounts for class imbalance by using a specialized loss function. The model, along with the datasets, is prepared for training using an accelerator to ensure compatibility with the available hardware. Training arguments are specified to control various aspects of the training process, such as the number of epochs, batch size, and evaluation strategy. The best model is saved based on the F1 score metric, and the tokenizer is also saved for future use. This setup not only aims to achieve high accuracy but also encourages reproducibility and ease of use by saving the trained model with a timestamped directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706e27d9-3de0-4f96-bcca-0904ec4f3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "def train_function_no_sweeps(train_dataset, test_dataset):\n",
    "    \n",
    "    # Set the LoRA config\n",
    "    config = {\n",
    "        \"lora_alpha\": 1, #try 0.5, 1, 2, ..., 16\n",
    "        \"lora_dropout\": 0.2,\n",
    "        \"lr\": 5.701568055793089e-04,\n",
    "        \"lr_scheduler_type\": \"cosine\",\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"per_device_train_batch_size\": 4,\n",
    "        \"r\": 2,\n",
    "        \"weight_decay\": 0.2,\n",
    "        # Add other hyperparameters as needed\n",
    "    }\n",
    "    # The base model you will train a LoRA on top of\n",
    "    model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"  \n",
    "    \n",
    "    # Define labels and model\n",
    "    id2label = {0: \"No binding site\", 1: \"Binding site\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(id2label), id2label=id2label, label2id=label2id)\n",
    "\n",
    "    # Convert the model into a PeftModel\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.TOKEN_CLS, \n",
    "        inference_mode=False, \n",
    "        r=config[\"r\"], \n",
    "        lora_alpha=config[\"lora_alpha\"], \n",
    "        target_modules=[\"query\", \"key\", \"value\"], # also try \"dense_h_to_4h\" and \"dense_4h_to_h\"\n",
    "        lora_dropout=config[\"lora_dropout\"], \n",
    "        bias=\"none\" # or \"all\" or \"lora_only\" \n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Use the accelerator\n",
    "    model = accelerator.prepare(model)\n",
    "    train_dataset = accelerator.prepare(train_dataset)\n",
    "    test_dataset = accelerator.prepare(test_dataset)\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    # Training setup\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"esm2_t12_35M-lora-binding-sites_{timestamp}\",\n",
    "        learning_rate=config[\"lr\"],\n",
    "        lr_scheduler_type=config[\"lr_scheduler_type\"],\n",
    "        gradient_accumulation_steps=1,\n",
    "        max_grad_norm=config[\"max_grad_norm\"],\n",
    "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"per_device_train_batch_size\"],\n",
    "        num_train_epochs=config[\"num_train_epochs\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        push_to_hub=False,\n",
    "        logging_dir=None,\n",
    "        logging_first_step=False,\n",
    "        logging_steps=200,\n",
    "        save_total_limit=7,\n",
    "        no_cuda=False,\n",
    "        seed=8893,\n",
    "        fp16=False\n",
    "    )\n",
    "        # report_to='wandb'\n",
    "    # )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train and Save Model\n",
    "    trainer.train()\n",
    "    save_path = os.path.join(\"lora_binding_sites\", f\"best_model_esm2_t12_35M_lora_{timestamp}\")\n",
    "    trainer.save_model(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7fcabd-3c74-47a7-9d60-e73ef02d3190",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_function_no_sweeps(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50718ff7-f436-445e-9535-174fc3819a53",
   "metadata": {},
   "source": [
    "## Evaluating the model performance:\n",
    "\n",
    "After training the ESM-2 model with LoRA for protein binding site prediction, it's crucial to evaluate its performance to ensure that it generalizes well to new data and does not suffer from overfitting. By doing so, we can compute metrics for both the training and test datasets. Ideally, the metrics for these datasets should be comparable, indicating that the model performs consistently across both seen and unseen data. A significant discrepancy where the training metrics surpass the test metrics might suggest overfitting, meaning the model has learned to memorize the training data rather than generalize. Conversely, if the test metrics are better than the training metrics, it could indicate underfitting, where the model has not fully captured the patterns in the training data and could benefit from additional training. The code calculates a suite of metrics, including accuracy, precision, recall, F1 score, AUC, and MCC, to provide a comprehensive evaluation of the model's performance. These metrics are derived by first predicting labels for the datasets, then flattening the predictions and labels to compute the scores, ensuring that padding and special tokens are excluded from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba1d59-fe66-4902-bfe2-a31c0a4ae1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import(\n",
    "    matthews_corrcoef, \n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    roc_auc_score\n",
    ")\n",
    "from peft import PeftModel\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Define paths to the LoRA and base models\n",
    "base_model_path = \"facebook/esm2_t12_35M_UR50D\"\n",
    "lora_model_path = \"nidhinthomas/esm2_t12_35M_lora_binding_sites\" # \"path/to/your/lora/model\" Replace with the correct path to your LoRA model\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(base_model_path)\n",
    "\n",
    "# Load the LoRA model\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "model = accelerator.prepare(model)  # Prepare the model using the accelerator\n",
    "\n",
    "# Define label mappings\n",
    "id2label = {0: \"No binding site\", 1: \"Binding site\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Define a function to compute the metrics\n",
    "def compute_metrics(dataset):\n",
    "    # Get the predictions using the trained model\n",
    "    trainer = Trainer(model=model, data_collator=data_collator)\n",
    "    predictions, labels, _ = trainer.predict(test_dataset=dataset)\n",
    "    \n",
    "    # Remove padding and special tokens\n",
    "    mask = labels != -100\n",
    "    true_labels = labels[mask].flatten()\n",
    "    flat_predictions = np.argmax(predictions, axis=2)[mask].flatten().tolist()\n",
    "\n",
    "    # Compute the metrics\n",
    "    accuracy = accuracy_score(true_labels, flat_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, flat_predictions, average='binary')\n",
    "    auc = roc_auc_score(true_labels, flat_predictions)\n",
    "    mcc = matthews_corrcoef(true_labels, flat_predictions)  # Compute the MCC\n",
    "    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"auc\": auc, \"mcc\": mcc}  # Include the MCC in the returned dictionary\n",
    "\n",
    "# Get the metrics for the training and test datasets\n",
    "train_metrics = compute_metrics(train_dataset)\n",
    "test_metrics = compute_metrics(test_dataset)\n",
    "\n",
    "train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9546045-9b02-41d3-ba78-167886922575",
   "metadata": {},
   "source": [
    "## Model Inference:\n",
    "\n",
    "now perform inference on new protein sequences of interest. To do this, you'll need to load your trained model and its corresponding tokenizer. The model should be set to evaluation mode to disable training-specific behaviors like dropout. You can then input a protein sequence, which the tokenizer will process into a format suitable for the model. The inference is carried out without updating the model's gradients, ensuring that the operation is purely for prediction. The model outputs logits, which are then converted into discrete predictions representing the likelihood of each token being a binding site. These predictions are mapped back to human-readable labels, distinguishing between binding and non-binding sites. By iterating over the sequence tokens and their associated predictions, you can examine the model's inference on a token-by-token basis, excluding any special padding or control tokens that are not part of the original sequence. This process allows you to leverage the power of the fine-tuned LoRA model to gain insights into the binding site propensities within your protein sequences of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d3cd25a-3936-4b78-83d2-17f44603e079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('M', 'No binding site')\n",
      "('A', 'No binding site')\n",
      "('V', 'No binding site')\n",
      "('P', 'No binding site')\n",
      "('E', 'No binding site')\n",
      "('T', 'No binding site')\n",
      "('R', 'No binding site')\n",
      "('P', 'No binding site')\n",
      "('N', 'Binding site')\n",
      "('H', 'Binding site')\n",
      "('T', 'Binding site')\n",
      "('I', 'Binding site')\n",
      "('Y', 'Binding site')\n",
      "('I', 'Binding site')\n",
      "('N', 'Binding site')\n",
      "('N', 'Binding site')\n",
      "('L', 'Binding site')\n",
      "('N', 'Binding site')\n",
      "('E', 'Binding site')\n",
      "('K', 'Binding site')\n",
      "('I', 'No binding site')\n",
      "('K', 'Binding site')\n",
      "('K', 'No binding site')\n",
      "('D', 'No binding site')\n",
      "('E', 'No binding site')\n",
      "('L', 'Binding site')\n",
      "('K', 'No binding site')\n",
      "('K', 'No binding site')\n",
      "('S', 'No binding site')\n",
      "('L', 'No binding site')\n",
      "('H', 'No binding site')\n",
      "('A', 'No binding site')\n",
      "('I', 'No binding site')\n",
      "('F', 'Binding site')\n",
      "('S', 'No binding site')\n",
      "('R', 'No binding site')\n",
      "('F', 'Binding site')\n",
      "('G', 'Binding site')\n",
      "('Q', 'No binding site')\n",
      "('I', 'No binding site')\n",
      "('L', 'No binding site')\n",
      "('D', 'No binding site')\n",
      "('I', 'No binding site')\n",
      "('L', 'Binding site')\n",
      "('V', 'Binding site')\n",
      "('S', 'Binding site')\n",
      "('R', 'Binding site')\n",
      "('S', 'Binding site')\n",
      "('L', 'Binding site')\n",
      "('K', 'Binding site')\n",
      "('M', 'Binding site')\n",
      "('R', 'Binding site')\n",
      "('G', 'Binding site')\n",
      "('Q', 'Binding site')\n",
      "('A', 'Binding site')\n",
      "('F', 'No binding site')\n",
      "('V', 'No binding site')\n",
      "('I', 'No binding site')\n",
      "('F', 'Binding site')\n",
      "('K', 'No binding site')\n",
      "('E', 'No binding site')\n",
      "('V', 'No binding site')\n",
      "('S', 'No binding site')\n",
      "('S', 'No binding site')\n",
      "('A', 'No binding site')\n",
      "('T', 'No binding site')\n",
      "('N', 'No binding site')\n",
      "('A', 'No binding site')\n",
      "('L', 'No binding site')\n",
      "('R', 'No binding site')\n",
      "('S', 'No binding site')\n",
      "('M', 'Binding site')\n",
      "('Q', 'No binding site')\n",
      "('G', 'Binding site')\n",
      "('F', 'Binding site')\n",
      "('P', 'Binding site')\n",
      "('F', 'Binding site')\n",
      "('Y', 'Binding site')\n",
      "('D', 'Binding site')\n",
      "('K', 'Binding site')\n",
      "('P', 'Binding site')\n",
      "('M', 'Binding site')\n",
      "('R', 'Binding site')\n",
      "('I', 'Binding site')\n",
      "('Q', 'Binding site')\n",
      "('Y', 'Binding site')\n",
      "('A', 'Binding site')\n",
      "('K', 'Binding site')\n",
      "('T', 'Binding site')\n",
      "('D', 'Binding site')\n",
      "('S', 'No binding site')\n",
      "('D', 'No binding site')\n",
      "('I', 'No binding site')\n",
      "('I', 'No binding site')\n",
      "('A', 'No binding site')\n",
      "('K', 'No binding site')\n",
      "('M', 'No binding site')\n",
      "('K', 'No binding site')\n",
      "('G', 'No binding site')\n",
      "('T', 'No binding site')\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Path to the saved LoRA model\n",
    "model_path = \"AmelieSchreiber/esm2_t12_35M_lora_binding_sites_v2_cp3\"\n",
    "# ESM2 base model\n",
    "base_model_path = \"facebook/esm2_t12_35M_UR50D\"\n",
    "\n",
    "# Load the model\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(base_model_path)\n",
    "loaded_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# Protein sequence for inference\n",
    "protein_sequence = \"MAVPETRPNHTIYINNLNEKIKKDELKKSLHAIFSRFGQILDILVSRSLKMRGQAFVIFKEVSSATNALRSMQGFPFYDKPMRIQYAKTDSDIIAKMKGT\"  # Replace with your actual sequence\n",
    "\n",
    "# Tokenize the sequence\n",
    "inputs = loaded_tokenizer(protein_sequence, return_tensors=\"pt\", truncation=True, max_length=1024, padding='max_length')\n",
    "\n",
    "# Run the model\n",
    "with torch.no_grad():\n",
    "    logits = loaded_model(**inputs).logits\n",
    "\n",
    "# Get predictions\n",
    "tokens = loaded_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])  # Convert input ids back to tokens\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Define labels\n",
    "id2label = {\n",
    "    0: \"No binding site\",\n",
    "    1: \"Binding site\"\n",
    "}\n",
    "\n",
    "# Print the predicted labels for each token\n",
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    if token not in ['<pad>', '<cls>', '<eos>']:\n",
    "        print((token, id2label[prediction]))\n",
    "        \n",
    "for token, prediction in zip(tokens, predictions[0].numpy()):\n",
    "    if prediction == 1:\n",
    "        print(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf582a-8f18-4dc3-9f2e-cd13f47f9909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import(\n",
    "    matthews_corrcoef, \n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    roc_auc_score\n",
    ")\n",
    "from peft import PeftModel\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Define paths to the LoRA and base models\n",
    "base_model_path = \"facebook/esm2_t12_35M_UR50D\"\n",
    "lora_model_path = \"AmelieSchreiber/esm2_t12_35M_lora_binding_sites_v2_cp3\" # \"path/to/your/lora/model\" Replace with the correct path to your LoRA model\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(base_model_path)\n",
    "\n",
    "# Load the LoRA model\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "model = accelerator.prepare(model)  # Prepare the model using the accelerator\n",
    "\n",
    "# Define label mappings\n",
    "id2label = {0: \"No binding site\", 1: \"Binding site\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Create a data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Define a function to compute the metrics\n",
    "def compute_metrics(dataset):\n",
    "    # Get the predictions using the trained model\n",
    "    trainer = Trainer(model=model, data_collator=data_collator)\n",
    "    predictions, labels, _ = trainer.predict(test_dataset=dataset)\n",
    "    \n",
    "    # Remove padding and special tokens\n",
    "    mask = labels != -100\n",
    "    true_labels = labels[mask].flatten()\n",
    "    flat_predictions = np.argmax(predictions, axis=2)[mask].flatten().tolist()\n",
    "\n",
    "    # Compute the metrics\n",
    "    accuracy = accuracy_score(true_labels, flat_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, flat_predictions, average='binary')\n",
    "    auc = roc_auc_score(true_labels, flat_predictions)\n",
    "    mcc = matthews_corrcoef(true_labels, flat_predictions)  # Compute the MCC\n",
    "    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"auc\": auc, \"mcc\": mcc}  # Include the MCC in the returned dictionary\n",
    "\n",
    "# Get the metrics for the training and test datasets\n",
    "train_metrics = compute_metrics(train_dataset)\n",
    "test_metrics = compute_metrics(test_dataset)\n",
    "\n",
    "train_metrics, test_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09276f-8024-4641-8274-ac26fc26acf5",
   "metadata": {},
   "source": [
    "The structure of the aforementioned protein with the predicted binding site is shown below. The binding site is highlighted in green. \n",
    "\n",
    "![esm2_binding_site](esm2_binding_site.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe44cf-9db2-40c0-b69a-081ba000a01e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm",
   "language": "python",
   "name": "esm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
